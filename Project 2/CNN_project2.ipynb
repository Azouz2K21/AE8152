{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import datetime\n",
    "from numpy.random import seed\n",
    "seed(1) # Fixing random seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Data and Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1600 images belonging to 4 classes.\n",
      "Found 800 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# dimensions of our images\n",
    "img_width, img_height = 100, 100   \n",
    "\n",
    "# Relative directory paths\n",
    "train_data_dir = 'Data/Train'\n",
    "validation_data_dir = 'Data/Validation'\n",
    "\n",
    "#Check for image format\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "\n",
    "#Rescale all images for training & Apply data augmentation for better training performance\n",
    "train_datagen = ImageDataGenerator(\n",
    "    # horizontal_flip=True,\n",
    "    rescale = 1. / 255,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2)\n",
    "\n",
    "#Rescale my images for voldiation. Data augementation should not be applied to validation dataset\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "batch_size = 32\n",
    "#Generate the scaling and data augmentation to the training dataset\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size = (img_width, img_height),\n",
    "    batch_size = batch_size,\n",
    "    class_mode = 'categorical')\n",
    "\n",
    "#Generate the scaling to the test dataset\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size = (img_width, img_height),\n",
    "    batch_size = batch_size,\n",
    "    class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Develop the CNN Architecture\n",
    "#HINT: Start with 2 sets of (Convolutional (relu activation) and Max. Pooling layers (2,2 size)), and 3 Dense layers\n",
    "input_shape = (batch_size,img_height,img_width,3)\n",
    "model = Sequential(\n",
    "    [\n",
    "        Conv2D(32,kernel_size=(7,7),activation=\"relu\",input_shape=input_shape[1:], name=\"layer1\"),\n",
    "        MaxPooling2D(pool_size=(2,2),name=\"layer2\"),\n",
    "        Conv2D(64,kernel_size=(7,7),activation=\"relu\",input_shape=input_shape[1:], name=\"layer3\"),\n",
    "        MaxPooling2D(pool_size=(2,2),name=\"layer4\"),\n",
    "        Conv2D(64,kernel_size=(5,5),activation=\"relu\",input_shape=input_shape[1:], name=\"layer5\"),\n",
    "        MaxPooling2D(pool_size=(2,2),name=\"layer6\"),\n",
    "        Flatten(name=\"layer7\"), \n",
    "        Flatten(name=\"layer8\"), \n",
    "        Dense(64, activation=\"relu\", name=\"layer9\"),\n",
    "        Dropout(0.2, name=\"layer10\"),\n",
    "        Dense(10, activation=\"relu\", name=\"layer11\"),\n",
    "        Dropout(0.2, name=\"layer12\"),\n",
    "        Dense(4, activation=\"softmax\", name=\"layer13\"),\n",
    "    ]\n",
    ")\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "#Compile model with loss function, optimizer, and target metrics\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer1 (Conv2D)             (None, 94, 94, 32)        4736      \n",
      "                                                                 \n",
      " layer2 (MaxPooling2D)       (None, 47, 47, 32)        0         \n",
      "                                                                 \n",
      " layer3 (Conv2D)             (None, 43, 43, 64)        51264     \n",
      "                                                                 \n",
      " layer4 (MaxPooling2D)       (None, 21, 21, 64)        0         \n",
      "                                                                 \n",
      " layer5 (Conv2D)             (None, 17, 17, 128)       204928    \n",
      "                                                                 \n",
      " layer6 (MaxPooling2D)       (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " layer7 (Flatten)            (None, 8192)              0         \n",
      "                                                                 \n",
      " layer8 (Flatten)            (None, 8192)              0         \n",
      "                                                                 \n",
      " layer9 (Dense)              (None, 64)                524352    \n",
      "                                                                 \n",
      " layer10 (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " layer11 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      " layer12 (Dropout)           (None, 10)                0         \n",
      "                                                                 \n",
      " layer13 (Dense)             (None, 4)                 44        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 785,974\n",
      "Trainable params: 785,974\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mo112\\AppData\\Local\\Temp\\ipykernel_23244\\3016212894.py:12: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "50/50 [==============================] - 40s 799ms/step - loss: 1.8672 - accuracy: 0.2775 - val_loss: 1.4783 - val_accuracy: 0.2500\n",
      "Epoch 2/20\n",
      "50/50 [==============================] - 39s 781ms/step - loss: 1.0308 - accuracy: 0.4938 - val_loss: 0.6008 - val_accuracy: 0.5000\n",
      "Epoch 3/20\n",
      "50/50 [==============================] - 39s 779ms/step - loss: 0.8225 - accuracy: 0.5625 - val_loss: 0.6049 - val_accuracy: 0.7500\n",
      "Epoch 4/20\n",
      "50/50 [==============================] - 39s 780ms/step - loss: 0.5944 - accuracy: 0.6794 - val_loss: 0.4959 - val_accuracy: 0.7100\n",
      "Epoch 5/20\n",
      "50/50 [==============================] - 39s 781ms/step - loss: 0.4743 - accuracy: 0.7237 - val_loss: 0.4567 - val_accuracy: 0.7412\n",
      "Epoch 6/20\n",
      "50/50 [==============================] - 39s 778ms/step - loss: 0.4585 - accuracy: 0.7312 - val_loss: 0.4124 - val_accuracy: 0.7500\n",
      "Epoch 7/20\n",
      "50/50 [==============================] - 39s 778ms/step - loss: 0.4437 - accuracy: 0.7394 - val_loss: 0.5052 - val_accuracy: 0.6837\n",
      "Epoch 8/20\n",
      "50/50 [==============================] - 39s 778ms/step - loss: 0.4047 - accuracy: 0.7475 - val_loss: 0.3544 - val_accuracy: 0.7500\n",
      "Epoch 9/20\n",
      "50/50 [==============================] - 39s 783ms/step - loss: 0.7390 - accuracy: 0.6231 - val_loss: 0.5659 - val_accuracy: 0.6275\n",
      "Epoch 10/20\n",
      "50/50 [==============================] - 39s 782ms/step - loss: 0.5060 - accuracy: 0.6900 - val_loss: 0.4946 - val_accuracy: 0.6775\n",
      "Epoch 11/20\n",
      "50/50 [==============================] - 39s 783ms/step - loss: 0.4096 - accuracy: 0.7563 - val_loss: 0.3685 - val_accuracy: 0.7500\n",
      "Epoch 12/20\n",
      "50/50 [==============================] - 39s 781ms/step - loss: 0.3918 - accuracy: 0.7513 - val_loss: 0.3622 - val_accuracy: 0.7500\n",
      "Epoch 13/20\n",
      "50/50 [==============================] - 39s 781ms/step - loss: 0.7193 - accuracy: 0.6856 - val_loss: 1.7551 - val_accuracy: 0.2500\n",
      "Epoch 14/20\n",
      "50/50 [==============================] - 39s 785ms/step - loss: 1.5098 - accuracy: 0.2525 - val_loss: 1.3907 - val_accuracy: 0.2500\n",
      "Epoch 15/20\n",
      "50/50 [==============================] - 39s 781ms/step - loss: 1.4140 - accuracy: 0.2438 - val_loss: 1.3883 - val_accuracy: 0.2500\n",
      "Epoch 16/20\n",
      "50/50 [==============================] - 39s 779ms/step - loss: 1.4016 - accuracy: 0.2425 - val_loss: 1.3877 - val_accuracy: 0.2500\n",
      "Epoch 17/20\n",
      "50/50 [==============================] - 39s 781ms/step - loss: 1.4071 - accuracy: 0.2412 - val_loss: 1.3875 - val_accuracy: 0.2500\n",
      "Epoch 18/20\n",
      "50/50 [==============================] - 39s 780ms/step - loss: 1.3991 - accuracy: 0.2463 - val_loss: 1.3955 - val_accuracy: 0.2500\n",
      "Epoch 19/20\n",
      "50/50 [==============================] - 39s 786ms/step - loss: 1.3960 - accuracy: 0.2356 - val_loss: 1.3889 - val_accuracy: 0.2500\n",
      "Epoch 20/20\n",
      "50/50 [==============================] - 39s 780ms/step - loss: 1.3914 - accuracy: 0.2494 - val_loss: 1.3872 - val_accuracy: 0.2500\n"
     ]
    }
   ],
   "source": [
    "#Training & Validation dataset sizes\n",
    "nb_train_samples = 1600\n",
    "nb_validation_samples = 800\n",
    "\n",
    "#Model hyperparameter\n",
    "epochs = 20\n",
    "\n",
    "#Attain summary of the model\n",
    "model.summary()\n",
    "\n",
    "#Training the Neural Network Architecture (Need to define the \"model\" when creating the architecture)\n",
    "history = model.fit_generator(\n",
    "          train_generator, #Uses the augmented train data generated\n",
    "          steps_per_epoch = nb_train_samples // batch_size, #Training steps taken per epoch\n",
    "          epochs = epochs, #Total number of epochs defined\n",
    "          validation_data = validation_generator, #Uses the scaled validation data\n",
    "          validation_steps = nb_validation_samples // batch_size) #Validation steps taken per epoch\n",
    "\n",
    "#Save model (Saves Architecture and Weights)\n",
    "model.save('saved_trained_models/trained_model_{0}.h5'.format(datetime.today().strftime(\"%Y-%m-%d\"))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Performance: Accuracy and Loss of Trainning and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the accuracy and loss for both training and validation\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(20)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cf0a35b10abfd47113fbef6d65f9124a5cffdde3d768458c2ee4bb9a43eeb5eb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
